{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1de136-2608-4583-b189-fa4595f969b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/li19/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/u/li19/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926424a1-a412-4dcd-a8fb-a31bf4d95d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-si-hf\",device_map = 'cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15b4ebf-e81e-484e-85b0-0d12d274c718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|█████████████████████████████████████████████████████████████████| 4/4 [06:24<00:00, 96.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user \\nWhat is shown in this image?assistant \\nThere is a red stop sign in the image.user \\nWhat about this image? How many cats do you see?assistant\\ntwo',\n",
       " 'user \\nWhat is shown in this image?assistant\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n",
    "\n",
    "# Get three different images\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image_stop = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_cats = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n",
    "image_snowman = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\n",
    "conversation_1 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "            ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"There is a red stop sign in the image.\"},\n",
    "            ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "conversation_2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n",
    "prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# We can simply feed images in the order they have to be used in the text prompt\n",
    "inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80714b8-74eb-439c-9bb5-18bf0d165832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlm",
   "language": "python",
   "name": "mmlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
