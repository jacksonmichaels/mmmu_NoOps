{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee894bf-fa82-4db1-b923-6612a715025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/li19/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "from transformers import LlavaOnevisionForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from typing import Optional, Tuple\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_grad_enabled(False)\n",
    "import seaborn as sns\n",
    "from types import MethodType\n",
    "# os.chdir(\"/home/jacksonmicha_umass_edu/multimodal-modality-conflict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f06e77-0379-403b-8c6a-63f0fba3d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noop(data_path, subject, split=\"validation\", noop_root=\"/u/li19/MMMU/mmmu-noop\", noOps='text'):\n",
    "    from datasets import load_dataset\n",
    "    # Load your dataset\n",
    "    ds = load_dataset(data_path, subject, split=split)\n",
    "    \n",
    "    if noOps == \"text\" or noOps == \"all\":\n",
    "        # Load the NoOp data from JSON\n",
    "        noop_file = os.path.join(noop_root, f\"mmmu_{split}_noop_insert_sentence.json\")\n",
    "        with open(noop_file, 'r') as f:\n",
    "            noop_data = json.load(f)\n",
    "        \n",
    "    if noOps == \"img\" or noOps == \"all\":\n",
    "        # Load the NoOp data from JSON\n",
    "        img_noop_file = os.path.join(noop_root, f\"val_img_NoOp_metadata.json\")\n",
    "        with open(img_noop_file, 'r') as f:\n",
    "            img_data = json.load(f)\n",
    "    \n",
    "    # Define a function that adds \"noop\" to each example\n",
    "    def add_noop(example, noOps):\n",
    "        example_id = example['id']\n",
    "        if noOps == \"text\" or noOps == \"all\":\n",
    "            # print(\"adding text noop\")\n",
    "            example['noop'] = noop_data.get(example_id, None)  # fallback if ID not found\n",
    "        if noOps == \"img\" or noOps == \"all\":\n",
    "            example['noop_imgs'] = True\n",
    "            img_datum = img_data[example_id]['image_paths']\n",
    "            image_names = [f\"image_{n}\" for n in range(1, 8)]\n",
    "            imgs = [(name, example[name]) for name in image_names if example[name] is not None]\n",
    "            for img_pair, imgNoOp in zip(imgs, img_datum):\n",
    "                name, img = img_pair\n",
    "                example[name] = Image.open(imgNoOp + \".png\")\n",
    "                # print(\"adding img noop: \", name)\n",
    "        return example\n",
    "\n",
    "    # Use .map() to apply the function and return a new dataset with the `noop` column\n",
    "    ds = ds.map(lambda x: add_noop(x, noOps))\n",
    "\n",
    "    return ds\n",
    "\n",
    "def prepare_NoOp(data_path, split, all_subs, noOps='text'):\n",
    "    print(f\"Loading MMMU dataset from {data_path} for split={split} ...\")\n",
    "    sub_dataset_list = [load_noop(data_path, subject, split=split, noOps=noOps) for subject in tqdm(all_subs)]\n",
    "    dataset = concatenate_datasets(sub_dataset_list)\n",
    "    print(\"NoOp Dataset loaded:\", dataset)\n",
    "    return dataset\n",
    "\n",
    "def prepare_dataset(data_path, split, all_subs):\n",
    "    print(f\"Loading MMMU dataset from {data_path} for split={split} ...\")\n",
    "    sub_dataset_list = [load_dataset(data_path, subject, split=split) for subject in tqdm(all_subs)]\n",
    "    dataset = concatenate_datasets(sub_dataset_list)\n",
    "    print(\"Dataset loaded:\", dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b295571f-97bc-4e72-ac3d-9e2fa827f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMMU dataset from MMMU/MMMU for split=validation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 30/30 [01:03<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoOp Dataset loaded: Dataset({\n",
      "    features: ['id', 'question', 'options', 'explanation', 'image_1', 'image_2', 'image_3', 'image_4', 'image_5', 'image_6', 'image_7', 'img_type', 'answer', 'topic_difficulty', 'question_type', 'subfield', 'noop', 'noop_imgs'],\n",
      "    num_rows: 900\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_subs = [\n",
    "    'Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art',\n",
    "    'Art_Theory','Basic_Medical_Science','Biology','Chemistry','Clinical_Medicine',\n",
    "    'Computer_Science','Design','Diagnostics_and_Laboratory_Medicine','Economics',\n",
    "    'Electronics','Energy_and_Power','Finance','Geography','History','Literature',\n",
    "    'Manage','Marketing','Materials','Math','Mechanical_Engineering','Music',\n",
    "    'Pharmacy','Physics','Psychology','Public_Health','Sociology'\n",
    "]\n",
    "noop = \"all\"\n",
    "data_path = \"MMMU/MMMU\"\n",
    "split = \"validation\"\n",
    "subs = all_subs\n",
    "if noop != \"none\":\n",
    "    dataset = prepare_NoOp(data_path, split, subs, noop)\n",
    "else:\n",
    "    dataset = prepare_dataset(data_path, split, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb62db9-0ff3-4113-8f11-ab9b192f630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n",
    "device = \"cuda:0\"\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ceb4de-e11a-4a40-b333-f288b121ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vision_attn(image):\n",
    "    instruct = \"you are an advanced question answer model, please answer this question to the best of your ability with a single letter in brackets like this [X].\"\n",
    "    question_text = \"Who created <image 1>?\"\n",
    "    answers = 'A: ARKHIP KUINII B: PAUL SIGNAC C: GEORGES SEURAT D: VALENTIN SEROV'\n",
    "    \n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": instruct},\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": question_text},\n",
    "        {\"type\": \"text\", \"text\": answers},\n",
    "    ]\n",
    "    response = {\"role\": \"user\", \"content\": content}\n",
    "    prompt = processor.apply_chat_template([response], add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    \n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 32,\n",
    "        \"temperature\": 1.0,\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"length_penalty\": 1.0,\n",
    "        \"early_stopping\": True,\n",
    "        \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_attentions\": True\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            **inputs, output_attentions=True\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed66e8b1-f39c-4226-a237-cefc1532b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'validation_Art_11',\n",
       " 'question': 'Who created <image 1>?',\n",
       " 'options': \"['ARKHIP KUINII', 'PAUL SIGNAC', 'GEORGES SEURAT', 'VALENTIN SEROV']\",\n",
       " 'explanation': '',\n",
       " 'image_1': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1078x920>,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'image_6': None,\n",
       " 'image_7': None,\n",
       " 'img_type': \"['Paintings']\",\n",
       " 'answer': 'C',\n",
       " 'topic_difficulty': 'Medium',\n",
       " 'question_type': 'multiple-choice',\n",
       " 'subfield': 'Drawing and Painting',\n",
       " 'noop': \"Who created <image 1>? The museum's café serves a delicious cappuccino.\",\n",
       " 'noop_imgs': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = dataset[100]['image_1']\n",
    "# valid, output, inputs = test_vision_attn(img)\n",
    "output = test_vision_attn(img)\n",
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e511e84b-0a30-4e85-bc92-61033cbe636d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = output.logits[0, -1].argmax()\n",
    "token = processor.batch_decode([idx])\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf9f5a-cf45-465d-98ad-6c3717d5bff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlm",
   "language": "python",
   "name": "mmlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
