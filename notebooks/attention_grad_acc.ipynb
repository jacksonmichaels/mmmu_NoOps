{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f5de058-49ae-49f9-a6ee-c36b0a2d97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llava_code import call_engine_llava, format_prompt_llava, load_model_llava\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from torchvision.transforms.functional import pil_to_tensor, to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee53c78-f5f1-488a-8ddd-e418b9ffe918",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 39.38 GiB of which 14.12 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.17 GiB is allocated by PyTorch, and 687.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-onevision-qwen2-0.5b-ov-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m----> 4\u001b[0m model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_llava\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MMMU/notebooks/llava_code.py:28\u001b[0m, in \u001b[0;36mload_model_llava\u001b[0;34m(model_path, device, dtype)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monevision\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_path:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlavaNextForConditionalGeneration, LlavaOnevisionForConditionalGeneration, AutoProcessor\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaOnevisionForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path,trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-v1.6-mistral\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_path:\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/transformers/modeling_utils.py:3099\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3095\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3096\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3097\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3098\u001b[0m         )\n\u001b[0;32m-> 3099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/data_folder/anaconda3/envs/mmlm/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 39.38 GiB of which 14.12 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.17 GiB is allocated by PyTorch, and 687.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n",
    "dtype = torch.float32\n",
    "model, processor = load_model_llava(model_id, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec6493-9580-4877-a10e-bb8a8e4c37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dog = True\n",
    "\n",
    "if use_dog:\n",
    "    raw_img = np.array(Image.open(\"low_res_dog.png\"))[:,:,:3]\n",
    "    conversation = [\n",
    "       {\n",
    "         \"role\": \"user\",\n",
    "         \"content\": [\n",
    "             # {\"type\": \"text\", \"text\": sample['caption']},\n",
    "             {\"type\": \"text\", \"text\": \"the dog is brown and running in the snow with trees behind\"},\n",
    "             {\"type\": \"text\", \"text\": \"what type of dog is in the picture\"},\n",
    "             {\"type\": \"image\"},\n",
    "           ],\n",
    "       },\n",
    "    ]\n",
    "else:\n",
    "    raw_img = np.array(sample['image'])\n",
    "    conversation = [\n",
    "       {\n",
    "         \"role\": \"user\",\n",
    "         \"content\": [\n",
    "             {\"type\": \"text\", \"text\": sample['caption']},\n",
    "             # {\"type\": \"text\", \"text\": \"the dog is brown and running in the snow with trees behind\"},\n",
    "             {\"type\": \"text\", \"text\": \"What color color  is the shape\"},\n",
    "             {\"type\": \"image\"},\n",
    "           ],\n",
    "       },\n",
    "    ]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=raw_img, text=prompt, return_tensors='pt').to(device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142d1d7-11aa-4e21-a906-045d5166fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_values'].requires_grad_(True)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "logits = outputs.logits[0]\n",
    "\n",
    "attention_maps = torch.stack(outputs.attentions)  # Shape: (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "tokens = [processor.batch_decode(iid) for iid in inputs.input_ids][0]\n",
    "image_mask = [token == \"<image>\" for token in tokens]\n",
    "\n",
    "# Extract attention to image tokens\n",
    "img_attn = attention_maps[:, 0, :, image_mask, :]  # (num_layers, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Only keep attention scores where the query or key is an image token\n",
    "img_attn = img_attn[:, :, :, image_mask]  # Adjust this indexing based on your token position handling\n",
    "\n",
    "mean_attn = img_attn.mean()  # Shape: (num_layers, num_heads)\n",
    "mean_attn.backward(retain_graph=True)\n",
    "grad = inputs['pixel_values'].grad[0].detach().clone()\n",
    "inputs['pixel_values'].grad.zero_()  # Reset gradients for next backward pass\n",
    "\"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63773d4c-e519-4380-920e-9240bc7ee0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = inputs['pixel_values'].cpu().clone().detach()[0,0] + 1\n",
    "img = torch.permute(img, (1,2,0)) / 2\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a4708-1a47-45e0-823d-9e92df987b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.norm(grad, dim=[0]).cpu()\n",
    "g = torch.permute(g, (1,2,0))\n",
    "g = (g - g.min()) / (g.max() - g.min())\n",
    "print(g.shape)\n",
    "\n",
    "plt.imshow(g ** 0.33, cmap=\"viridis\", interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f6b16-fb12-4acd-9a4d-4434afba0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_noisy_gradients(base_img, prompt, N=10, noise_std=10.0):\n",
    "    \"\"\"\n",
    "    Generate N noisy versions of base_img by adding Gaussian noise,\n",
    "    run each image through the model with the given prompt, and return\n",
    "    a tensor of gradients computed from the mean image attention.\n",
    "    \n",
    "    Args:\n",
    "        base_img (np.array): The original image (e.g., a NumPy array).\n",
    "        prompt (str): The prompt string produced by processor.apply_chat_template.\n",
    "        N (int): Number of noisy images to generate.\n",
    "        noise_std (float): Standard deviation of the Gaussian noise.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (N, C, H, W) containing the gradient values.\n",
    "    \"\"\"\n",
    "    grad_list = []\n",
    "    \n",
    "    for i in tqdm(range(N)):\n",
    "        # Create a noisy version of the base image\n",
    "        noise = np.random.normal(loc=0.0, scale=noise_std, size=base_img.shape)\n",
    "        noisy_img = base_img + noise\n",
    "        # Clip to valid pixel range and convert back to original dtype\n",
    "        noisy_img = np.clip(noisy_img, 0, 255).astype(base_img.dtype)\n",
    "        \n",
    "        # Process the noisy image with the same text prompt\n",
    "        inputs = processor(images=noisy_img, text=prompt, return_tensors='pt').to(0, torch.float32)\n",
    "        inputs['pixel_values'].requires_grad_(True)\n",
    "        \n",
    "        # Forward pass with the model, requesting attention maps\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        \n",
    "        # Stack all attention maps from different layers: shape (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "        attention_maps = torch.stack(outputs.attentions)\n",
    "        \n",
    "        # Determine which tokens correspond to the image (assumes \"<image>\" tokens)\n",
    "        tokens = [processor.batch_decode(iid) for iid in inputs.input_ids][0]\n",
    "        image_mask = [token == \"<image>\" for token in tokens]\n",
    "        \n",
    "        # Extract attention scores involving image tokens as queries and keys\n",
    "        img_attn = attention_maps[:, 0, :, image_mask, :]\n",
    "        img_attn = img_attn[:, :, :, image_mask]\n",
    "        \n",
    "        # Compute the mean attention value and backpropagate\n",
    "        mean_attn = img_attn.mean()\n",
    "        mean_attn.backward(retain_graph=True)\n",
    "        \n",
    "        # Get the gradient of the image pixels (assumes batch size = 1)\n",
    "        grad = inputs['pixel_values'].grad[0].detach().clone()\n",
    "        # Zero out gradients before the next iteration\n",
    "        inputs['pixel_values'].grad.zero_()\n",
    "        \n",
    "        grad_list.append(grad)\n",
    "    \n",
    "    # Stack gradients along a new dimension: shape (N, C, H, W)\n",
    "    grad_tensor = torch.stack(grad_list)\n",
    "    return grad_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82b372-5c4a-421b-9cd6-cc70662d09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "gradients = compute_noisy_gradients(np.array(raw_img), prompt, N=50, noise_std=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2f8b6-e590-4d79-9cb8-49de035ed44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = gradients.mean(dim=[0]).cpu()\n",
    "mean = torch.permute(mean, (0, 2,3,1))\n",
    "\n",
    "g = torch.norm(mean, dim=[0]).cpu()\n",
    "# g = torch.permute(g, (1,2,0))\n",
    "g = (g - g.min()) / (g.max() - g.min())\n",
    "print(g.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(g ** 0.33, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "axs[1].imshow(torch.norm(g, dim=[2]) ** 0.33, cmap=\"viridis\", interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2e6a6-789e-4167-a30d-7828b32a1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_integrated_gradients_torch(base_img, prompt, baseline=None, steps=50, do_rescale=True):\n",
    "    \"\"\"\n",
    "    Compute integrated gradients for base_img with respect to the model's output.\n",
    "    \n",
    "    Integrated gradients are computed by linearly interpolating between a baseline image\n",
    "    and the target image, summing gradients along the path, averaging them, and scaling\n",
    "    by the difference between the input and the baseline.\n",
    "    \n",
    "    Args:\n",
    "        base_img (torch.Tensor): The target image tensor with values in the 0–255 range.\n",
    "        prompt (str): The text prompt produced by processor.apply_chat_template.\n",
    "        baseline (torch.Tensor, optional): The baseline image tensor; if None, a tensor of zeros is used.\n",
    "        steps (int): The number of interpolation steps between the baseline and base_img.\n",
    "        do_rescale (bool): Whether to rescale the image; for images in 0–255, set to True.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of integrated gradients with the same shape as base_img.\n",
    "    \"\"\"\n",
    "    # Use a black image (all zeros) as the baseline if none is provided.\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(base_img).to(device, dtype)\n",
    "    \n",
    "    integrated_grad = 0.0\n",
    "    # Interpolate from baseline to base_img over the specified number of steps.\n",
    "    for alpha in tqdm(torch.linspace(0.01, 1, steps, device=base_img.device)):\n",
    "        # Create an interpolated image.\n",
    "        scaled_img = baseline + alpha * (base_img - baseline)\n",
    "        \n",
    "        # Process the scaled image; use do_rescale=True since base_img is in 0–255.\n",
    "        inputs = processor(\n",
    "            images=to_pil_image(scaled_img), \n",
    "            text=prompt, \n",
    "            return_tensors='pt', \n",
    "        ).to(0, torch.float32)\n",
    "        inputs['pixel_values'].requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through the model, retrieving attention outputs.\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        attention_maps = torch.stack(outputs.attentions)  # (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Identify tokens corresponding to the image.\n",
    "        tokens = [processor.batch_decode(iid) for iid in inputs.input_ids][0]\n",
    "        image_mask = [token == \"<image>\" for token in tokens]\n",
    "        \n",
    "        # Extract attention scores where both query and key correspond to image tokens.\n",
    "        img_attn = attention_maps[:, 0, :, image_mask, :]\n",
    "        img_attn = img_attn[:, :, :, image_mask]\n",
    "        \n",
    "        # Compute the mean attention and backpropagate.\n",
    "        mean_attn = img_attn.mean()\n",
    "        mean_attn.backward(retain_graph=True)\n",
    "        \n",
    "        # Retrieve the gradients for this step.\n",
    "        grad = inputs['pixel_values'].grad[0].detach().clone()\n",
    "        inputs['pixel_values'].grad.zero_()\n",
    "        \n",
    "        integrated_grad += grad\n",
    "    \n",
    "    # Average the gradients over all interpolation steps.\n",
    "    integrated_grad /= steps\n",
    "    \n",
    "    # Scale the integrated gradients by the difference between the base image and baseline.\n",
    "    # ig = (base_img - baseline) * integrated_grad\n",
    "    return integrated_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20326a3e-848a-408a-92cc-e386f9ae124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tImg = pil_to_tensor(raw_img).to(device, dtype)\n",
    "tImg = torch.tensor(raw_img).to(device, dtype).permute((2,0,1))\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "ig_tensor = compute_integrated_gradients_torch(tImg, prompt, baseline=None, steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617128ad-d3c2-487f-8eb9-4c4c153bc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig_tensor.mean(dim=[0]).cpu()\n",
    "# mean = torch.permute(mean, (1,2,0))\n",
    "\n",
    "g = torch.norm(mean, dim=[0]).cpu()\n",
    "# g = torch.permute(g, (1,2,0))\n",
    "g = (g - g.min()) / (g.max() - g.min())\n",
    "\n",
    "plt.imshow(g ** 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f025cb1-25e3-4719-9c8e-bd8666fa3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_integrated_gradients_torch(base_img, prompt, baseline=None, steps=50, do_rescale=True):\n",
    "    \"\"\"\n",
    "    Compute integrated gradients for base_img using LLaVA-OneVision.\n",
    "    \n",
    "    This function creates a batch of interpolated images (from baseline to base_img) and processes \n",
    "    them with the processor as follows:\n",
    "    \n",
    "        inputs = processor(\n",
    "                     images=pil_images,\n",
    "                     text=[prompt] * len(pil_images),\n",
    "                     return_tensors='pt',\n",
    "                 )\n",
    "    \n",
    "    Then, it performs a forward pass with output_attentions=True to obtain attention maps.\n",
    "    Integrated gradients are computed by backpropagating an overall mean attention scalar \n",
    "    and per-layer/per-head attention scalars.\n",
    "    \n",
    "    Args:\n",
    "        base_img (torch.Tensor): Target image tensor (values in 0–255) with shape (C, H, W).\n",
    "        prompt (str): The prompt (e.g. produced by processor.apply_chat_template) to be paired with every image.\n",
    "        baseline (torch.Tensor, optional): Baseline image tensor; if None, uses a black image.\n",
    "        steps (int): Number of interpolation steps.\n",
    "        do_rescale (bool): Whether to apply rescaling (e.g. 0–255 to 0–1) if required.\n",
    "        \n",
    "    Returns:\n",
    "        integrated_grad (torch.Tensor): Overall integrated gradient (same shape as base_img).\n",
    "        pixel_gradients_per_layer_head (torch.Tensor): Per-layer/per-head gradients (shape: (num_layers, num_heads, C, H, W)).\n",
    "        grad_magnitude_per_layer_head (torch.Tensor): Gradient magnitudes per layer/head (shape: (num_layers, num_heads, H, W)).\n",
    "    \"\"\"\n",
    "    # Ensure the prompt is a string.\n",
    "    if isinstance(prompt, list):\n",
    "        prompt = \" \".join(map(str, prompt))\n",
    "    elif not isinstance(prompt, str):\n",
    "        prompt = str(prompt)\n",
    "    \n",
    "    # Use a black image as baseline if none is provided.\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(base_img).to(device, dtype)\n",
    "    \n",
    "    # Create a batch of interpolated images.\n",
    "    base_img_batch = base_img.unsqueeze(0)      # (1, C, H, W)\n",
    "    baseline_batch = baseline.unsqueeze(0)        # (1, C, H, W)\n",
    "    alphas = torch.linspace(0.01, 1.0, steps, device=base_img.device).view(-1, 1, 1, 1)  # (steps, 1, 1, 1)\n",
    "    scaled_imgs = baseline_batch + alphas * (base_img_batch - baseline_batch)  # (steps, C, H, W)\n",
    "    \n",
    "    # Optionally apply rescaling if required (e.g., converting 0–255 to 0–1).\n",
    "    if do_rescale:\n",
    "        # For example: scaled_imgs = scaled_imgs / 255.0\n",
    "        pass\n",
    "    \n",
    "    # Convert each interpolated image to a PIL image.\n",
    "    pil_images = [to_pil_image(img) for img in scaled_imgs]\n",
    "    \n",
    "    # Process the batch of images paired with the prompt.\n",
    "    # The processor expects a list of images and a corresponding list of text prompts.\n",
    "    inputs = processor(\n",
    "        images=pil_images,\n",
    "        text=[prompt] * len(pil_images),\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    # Move each tensor in the inputs to the target device.\n",
    "    # For pixel_values, we use torch.float32; for others (e.g. input_ids), we leave their dtypes unchanged.\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            if k == 'pixel_values':\n",
    "                inputs[k] = v.to(model.device, torch.float32)\n",
    "            else:\n",
    "                inputs[k] = v.to(model.device)\n",
    "    \n",
    "    # Enable gradient computation on the pixel values.\n",
    "    inputs['pixel_values'].requires_grad_(True)  # shape: (steps, C, H, W)\n",
    "    \n",
    "    # Run a single forward pass through the model with output_attentions=True.\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    # Stack attention outputs from all layers into one tensor.\n",
    "    # Expected shape: (num_layers, steps, num_heads, seq_len, seq_len)\n",
    "    attention_maps = torch.stack(outputs.attentions)\n",
    "    \n",
    "    # Use tokens from the first example (assuming the same prompt for every image).\n",
    "    tokens = processor.batch_decode(inputs['input_ids'][0:1])[0]\n",
    "    image_mask = [token == \"<image>\" for token in tokens]\n",
    "    \n",
    "    # Extract the attention corresponding to image tokens.\n",
    "    img_attn = attention_maps[:, :, :, image_mask, :]   # (num_layers, steps, num_heads, num_img_tokens, seq_len)\n",
    "    img_attn = img_attn[:, :, :, :, image_mask]          # (num_layers, steps, num_heads, num_img_tokens, num_img_tokens)\n",
    "    \n",
    "    # --- Overall Integrated Gradient ---\n",
    "    overall_mean_attn = img_attn.mean()  # Scalar average over all dimensions.\n",
    "    overall_mean_attn.backward(retain_graph=True)\n",
    "    # Average gradients over the interpolation steps.\n",
    "    integrated_grad = inputs['pixel_values'].grad.mean(dim=0).detach().clone() / steps\n",
    "    inputs['pixel_values'].grad.zero_()  # Reset gradients.\n",
    "    \n",
    "    # --- Per-Layer, Per-Head Gradients ---\n",
    "    # Average attention over steps and token positions -> shape: (num_layers, num_heads)\n",
    "    mean_attention_per_layer_head = img_attn.mean(dim=[1, 3, 4])\n",
    "    \n",
    "    pixel_gradients_per_layer_head_list = []\n",
    "    num_layers, num_heads = mean_attention_per_layer_head.shape\n",
    "    for layer in tqdm(range(num_layers), desc=\"Computing per-layer/head gradients\"):\n",
    "        for head in range(num_heads):\n",
    "            scalar = mean_attention_per_layer_head[layer, head]\n",
    "            scalar.backward(retain_graph=True)\n",
    "            grad_avg = inputs['pixel_values'].grad.mean(dim=0).detach().clone() / steps\n",
    "            pixel_gradients_per_layer_head_list.append(grad_avg)\n",
    "            inputs['pixel_values'].grad.zero_()\n",
    "    \n",
    "    # Reshape to (num_layers, num_heads, C, H, W)\n",
    "    pixel_gradients_per_layer_head = torch.stack(pixel_gradients_per_layer_head_list).view(\n",
    "        num_layers, num_heads, *inputs['pixel_values'].shape[1:]\n",
    "    )\n",
    "    \n",
    "    # Compute gradient magnitudes (norm over the channel dimension).\n",
    "    grad_magnitude_per_layer_head = torch.norm(pixel_gradients_per_layer_head, dim=2)\n",
    "    \n",
    "    return integrated_grad, pixel_gradients_per_layer_head, grad_magnitude_per_layer_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b230d5-8cb5-4c8a-9b42-121cc3852932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume raw_img is already loaded (e.g., as a NumPy array or PIL image)\n",
    "# and device, dtype are defined.\n",
    "tImg = torch.tensor(raw_img).to(device, dtype).permute((2, 0, 1))\n",
    "# Call the new integrated gradients function.\n",
    "ig_tensor, per_head_gradients, grad_magnitude = compute_integrated_gradients_torch(\n",
    "    tImg,\n",
    "    conversation,\n",
    "    baseline=None,\n",
    "    steps=50,\n",
    "    do_rescale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e322e9-8b4b-4ff4-8818-5e3ca614ca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203599e3-6b2c-413b-a0ea-eb2b1e9baa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlm",
   "language": "python",
   "name": "mmlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
